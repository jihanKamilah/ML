{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA00wBE2Ntdm"
   },
   "source": [
    "### Import TFRS\n",
    "\n",
    "First, install and import TFRS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yzAaM85Z12D",
    "outputId": "b1b0e033-85a0-4d4c-955d-0d88bfcc7aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q --upgrade tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n3oYt3R6Nr9l"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCxQ1CZcO2wh"
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-mxBYjdO5m7"
   },
   "outputs": [],
   "source": [
    "# # Ratings data.\n",
    "# ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
    "# # Features of all the available movies.\n",
    "# movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
    "\n",
    "# # Select the basic features.\n",
    "# ratings = ratings.map(lambda x: {\n",
    "#     \"movie_title\": x[\"movie_title\"],\n",
    "#     \"user_id\": x[\"user_id\"]\n",
    "# })\n",
    "# movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AoQXM8YqdDNa"
   },
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = pd.read_csv('../Downloads/user_rating.csv')\n",
    "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings))\n",
    "# Features of all the available movies.\n",
    "movies = pd.read_csv('../Downloads/recipe_dataset.csv')\n",
    "movies = tf.data.Dataset.from_tensor_slices(dict(movies))\n",
    "\n",
    "# Select the basic features.\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"Title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W0HSfmSNCWm"
   },
   "source": [
    "Build vocabularies to convert user ids and movie titles into integer indices for embedding layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9I1VTEjHzpfX"
   },
   "outputs": [],
   "source": [
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "# user_ids_vocabulary.adapt(ratings.map(lambda x: tf.strings.as_string(x[\"user_id\"])))\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrch6rVBOB9Q"
   },
   "source": [
    "### Define a model\n",
    "\n",
    "We can define a TFRS model by inheriting from `tfrs.Model` and implementing the `compute_loss` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e5dNbDZwOIHR"
   },
   "outputs": [],
   "source": [
    "class MovieLensModel(tfrs.Model):\n",
    "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "  # these are still plain Keras Models.\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      user_model: tf.keras.Model,\n",
    "      movie_model: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "    super().__init__()\n",
    "\n",
    "    # Set up user and movie representations.\n",
    "    self.user_model = user_model\n",
    "    self.movie_model = movie_model\n",
    "\n",
    "    # Set up a retrieval task.\n",
    "    self.task = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # Define how the loss is computed.\n",
    "\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "    return self.task(user_embeddings, movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdwtgUCEOI8y"
   },
   "source": [
    "Define the two models and the retrieval task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvtnUN6aUY4U",
    "outputId": "8572aa11-f183-42f0-e031-14b1cd80acec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    }
   ],
   "source": [
    "# Define user and movie models.\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 64)\n",
    "])\n",
    "movie_model = tf.keras.Sequential([\n",
    "    movie_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 64)\n",
    "])\n",
    "\n",
    "# Define your objectives.\n",
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "    movies.batch(128).map(movie_model)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMV0HpzmJGWk"
   },
   "source": [
    "\n",
    "### Fit and evaluate it.\n",
    "\n",
    "Create the model, train it, and generate predictions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2tQDhqkOKf1",
    "outputId": "ed563f76-a830-43cd-9aff-93bac367776d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6/6 [==============================] - 4s 523ms/step - factorized_top_k/top_1_categorical_accuracy: 4.5314e-05 - factorized_top_k/top_5_categorical_accuracy: 9.0629e-05 - factorized_top_k/top_10_categorical_accuracy: 9.0629e-05 - factorized_top_k/top_50_categorical_accuracy: 9.9692e-04 - factorized_top_k/top_100_categorical_accuracy: 0.0033 - loss: 27679.5469 - regularization_loss: 0.0000e+00 - total_loss: 27679.5469\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 3s 538ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0141 - factorized_top_k/top_5_categorical_accuracy: 0.0298 - factorized_top_k/top_10_categorical_accuracy: 0.0520 - factorized_top_k/top_50_categorical_accuracy: 0.1280 - factorized_top_k/top_100_categorical_accuracy: 0.1797 - loss: 25341.3809 - regularization_loss: 0.0000e+00 - total_loss: 25341.3809\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 3s 525ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0397 - factorized_top_k/top_5_categorical_accuracy: 0.3370 - factorized_top_k/top_10_categorical_accuracy: 0.4664 - factorized_top_k/top_50_categorical_accuracy: 0.6912 - factorized_top_k/top_100_categorical_accuracy: 0.7921 - loss: 17493.8165 - regularization_loss: 0.0000e+00 - total_loss: 17493.8165\n",
      "Top 3 recommendations for user 42: [b'Tahu & Mie Crispy' b'Sop kikil (kaki kambing)'\n",
      " b'Ikan Sarden Sambel Merah No MSG']\n"
     ]
    }
   ],
   "source": [
    "# Create a retrieval model.\n",
    "model = MovieLensModel(user_model, movie_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# Train for 3 epochs.\n",
    "model.fit(ratings.batch(4096), epochs=3)\n",
    "\n",
    "# Use brute-force search to set up retrieval using the trained representations.\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "index.index_from_dataset(\n",
    "    movies.batch(100).map(lambda title: (title, model.movie_model(title))))\n",
    "\n",
    "# Get some recommendations.\n",
    "_, titles = index(np.array([\"42\"]))\n",
    "print(f\"Top 3 recommendations for user 42: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "neJAJVwbReNd",
    "outputId": "ae342d2a-e3d1-47c4-d5b4-86765113c998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 recommendations for user 1551: [b'Tahu & Mie Crispy' b'Sop kikil (kaki kambing)'\n",
      " b'Ikan Sarden Sambel Merah No MSG']\n"
     ]
    }
   ],
   "source": [
    "# Get some recommendations.\n",
    "_, titles = index(np.array([\"0\"]))\n",
    "print(f\"Top 3 recommendations for user 1551: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zug8L75KhF7w",
    "outputId": "24223884-83c0-42ff-cd2b-5ee05f4b52cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 recommendations for user 15051: [b'Tahu & Mie Crispy' b'Sop kikil (kaki kambing)'\n",
      " b'Ikan Sarden Sambel Merah No MSG']\n"
     ]
    }
   ],
   "source": [
    "# Get some recommendations.\n",
    "_, titles = index(np.array([\"15051\"]))\n",
    "print(f\"Top 3 recommendations for user 15051: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiI0YlOrhZo_",
    "outputId": "a314d6f1-b1c9-45a9-8758-97f2e399cd75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 recommendations for user 15051: [b'Tahu & Mie Crispy' b'Sop kikil (kaki kambing)'\n",
      " b'Ikan Sarden Sambel Merah No MSG']\n"
     ]
    }
   ],
   "source": [
    "# Get some recommendations.\n",
    "_, titles = index(np.array([\"15052\"]))\n",
    "print(f\"Top 3 recommendations for user 15051: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JM-nQjrugoMy",
    "outputId": "221bc6c8-807a-4cc7-8631-9b3823b789e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 recommendations for user 1551: [b'Tumis Cumi asin pedas campur tahu..' b'Thai Beef Salad'\n",
      " b'Bubur Ayam Bumbu Kuning']\n"
     ]
    }
   ],
   "source": [
    "# Get some recommendations.\n",
    "_, titles = index(np.array([\"200\"]))\n",
    "print(f\"Top 3 recommendations for user 1551: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCEhIuvQhXqt",
    "outputId": "41e84e67-46b3-45c6-9aeb-fb99bb94fb86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 recommendations for user 15051: [b'Sate Kambing Empuk Sambel Kecap' b'Bola Bola isi tahu'\n",
      " b'Tempe Kemangi Bu-Ris Enaak']\n"
     ]
    }
   ],
   "source": [
    "# Get some recommendations.\n",
    "_, titles = index(np.array([\"100\"]))\n",
    "print(f\"Top 3 recommendations for user 15051: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytH4D9F_hoBo",
    "outputId": "8ef27636-a427-439c-ad9d-e250e70b9318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 recommendations for user 15051: [b'Kari telur tempe tahu' b'Chicken katsu' b'Chicken katsu']\n"
     ]
    }
   ],
   "source": [
    "# Get some recommendations.\n",
    "_, titles = index(np.array([\"1000\"]))\n",
    "print(f\"Top 3 recommendations for user 15051: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVFtHIZo9GxE",
    "outputId": "7f87f97a-ea5a-4733-a853-b036680386ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 recommendations for user 15051: [b'Asam pedas ceker tulang ayam kuah mercon' b'Bandeng Bakar tulang lunak'\n",
      " b'Ca brokoli ayam']\n"
     ]
    }
   ],
   "source": [
    "# Get some recommendations.\n",
    "_, titles = index(np.array([\"150\"]))\n",
    "print(f\"Top 3 recommendations for user 15051: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRJyw1pl9H9c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlEoHIfYQmrY"
   },
   "source": [
    "## **Calculate validation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SWyngLodSQfO",
    "outputId": "427d4e62-22c4-43f2-d61a-1ee8ffdd8f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 4s 773ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 5.6644e-05 - factorized_top_k/top_10_categorical_accuracy: 1.1329e-04 - factorized_top_k/top_50_categorical_accuracy: 3.9651e-04 - factorized_top_k/top_100_categorical_accuracy: 0.0014 - loss: 33924.4477 - regularization_loss: 0.0000e+00 - total_loss: 33924.4477\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 3s 756ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0065 - factorized_top_k/top_5_categorical_accuracy: 0.0411 - factorized_top_k/top_10_categorical_accuracy: 0.0709 - factorized_top_k/top_50_categorical_accuracy: 0.1740 - factorized_top_k/top_100_categorical_accuracy: 0.2345 - loss: 29638.8902 - regularization_loss: 0.0000e+00 - total_loss: 29638.8902\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 3s 722ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0544 - factorized_top_k/top_5_categorical_accuracy: 0.3303 - factorized_top_k/top_10_categorical_accuracy: 0.4939 - factorized_top_k/top_50_categorical_accuracy: 0.7163 - factorized_top_k/top_100_categorical_accuracy: 0.7865 - loss: 20007.5582 - regularization_loss: 0.0000e+00 - total_loss: 20007.5582\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 3s 741ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1506 - factorized_top_k/top_5_categorical_accuracy: 0.5897 - factorized_top_k/top_10_categorical_accuracy: 0.7042 - factorized_top_k/top_50_categorical_accuracy: 0.8531 - factorized_top_k/top_100_categorical_accuracy: 0.8880 - loss: 14496.4916 - regularization_loss: 0.0000e+00 - total_loss: 14496.4916\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 3s 741ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2176 - factorized_top_k/top_5_categorical_accuracy: 0.6311 - factorized_top_k/top_10_categorical_accuracy: 0.7473 - factorized_top_k/top_50_categorical_accuracy: 0.8891 - factorized_top_k/top_100_categorical_accuracy: 0.9208 - loss: 13048.5891 - regularization_loss: 0.0000e+00 - total_loss: 13048.5891\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 3s 737ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2347 - factorized_top_k/top_5_categorical_accuracy: 0.6948 - factorized_top_k/top_10_categorical_accuracy: 0.8123 - factorized_top_k/top_50_categorical_accuracy: 0.9435 - factorized_top_k/top_100_categorical_accuracy: 0.9631 - loss: 9933.4721 - regularization_loss: 0.0000e+00 - total_loss: 9933.4721\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 3s 784ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2393 - factorized_top_k/top_5_categorical_accuracy: 0.7313 - factorized_top_k/top_10_categorical_accuracy: 0.8554 - factorized_top_k/top_50_categorical_accuracy: 0.9724 - factorized_top_k/top_100_categorical_accuracy: 0.9845 - loss: 8064.3437 - regularization_loss: 0.0000e+00 - total_loss: 8064.3437\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 3s 762ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2445 - factorized_top_k/top_5_categorical_accuracy: 0.7584 - factorized_top_k/top_10_categorical_accuracy: 0.8787 - factorized_top_k/top_50_categorical_accuracy: 0.9845 - factorized_top_k/top_100_categorical_accuracy: 0.9936 - loss: 7131.5990 - regularization_loss: 0.0000e+00 - total_loss: 7131.5990\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 3s 750ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2467 - factorized_top_k/top_5_categorical_accuracy: 0.7756 - factorized_top_k/top_10_categorical_accuracy: 0.8979 - factorized_top_k/top_50_categorical_accuracy: 0.9914 - factorized_top_k/top_100_categorical_accuracy: 0.9977 - loss: 6507.1706 - regularization_loss: 0.0000e+00 - total_loss: 6507.1706\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 3s 754ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2468 - factorized_top_k/top_5_categorical_accuracy: 0.7768 - factorized_top_k/top_10_categorical_accuracy: 0.8989 - factorized_top_k/top_50_categorical_accuracy: 0.9931 - factorized_top_k/top_100_categorical_accuracy: 0.9986 - loss: 6052.6699 - regularization_loss: 0.0000e+00 - total_loss: 6052.6699\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 3s 757ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2453 - factorized_top_k/top_5_categorical_accuracy: 0.7917 - factorized_top_k/top_10_categorical_accuracy: 0.9109 - factorized_top_k/top_50_categorical_accuracy: 0.9943 - factorized_top_k/top_100_categorical_accuracy: 0.9995 - loss: 5643.2834 - regularization_loss: 0.0000e+00 - total_loss: 5643.2834\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 3s 756ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2456 - factorized_top_k/top_5_categorical_accuracy: 0.7925 - factorized_top_k/top_10_categorical_accuracy: 0.9118 - factorized_top_k/top_50_categorical_accuracy: 0.9958 - factorized_top_k/top_100_categorical_accuracy: 0.9998 - loss: 5391.4659 - regularization_loss: 0.0000e+00 - total_loss: 5391.4659\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 3s 750ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2381 - factorized_top_k/top_5_categorical_accuracy: 0.8027 - factorized_top_k/top_10_categorical_accuracy: 0.9193 - factorized_top_k/top_50_categorical_accuracy: 0.9953 - factorized_top_k/top_100_categorical_accuracy: 0.9997 - loss: 5149.4561 - regularization_loss: 0.0000e+00 - total_loss: 5149.4561\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 3s 771ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2377 - factorized_top_k/top_5_categorical_accuracy: 0.8040 - factorized_top_k/top_10_categorical_accuracy: 0.9193 - factorized_top_k/top_50_categorical_accuracy: 0.9965 - factorized_top_k/top_100_categorical_accuracy: 0.9999 - loss: 4983.6198 - regularization_loss: 0.0000e+00 - total_loss: 4983.6198\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 3s 769ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2308 - factorized_top_k/top_5_categorical_accuracy: 0.8099 - factorized_top_k/top_10_categorical_accuracy: 0.9232 - factorized_top_k/top_50_categorical_accuracy: 0.9968 - factorized_top_k/top_100_categorical_accuracy: 0.9999 - loss: 4837.1310 - regularization_loss: 0.0000e+00 - total_loss: 4837.1310\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 3s 751ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2303 - factorized_top_k/top_5_categorical_accuracy: 0.8101 - factorized_top_k/top_10_categorical_accuracy: 0.9215 - factorized_top_k/top_50_categorical_accuracy: 0.9968 - factorized_top_k/top_100_categorical_accuracy: 0.9999 - loss: 4747.1674 - regularization_loss: 0.0000e+00 - total_loss: 4747.1674\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 3s 770ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2249 - factorized_top_k/top_5_categorical_accuracy: 0.8143 - factorized_top_k/top_10_categorical_accuracy: 0.9256 - factorized_top_k/top_50_categorical_accuracy: 0.9968 - factorized_top_k/top_100_categorical_accuracy: 0.9999 - loss: 4667.7247 - regularization_loss: 0.0000e+00 - total_loss: 4667.7247\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 3s 755ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2209 - factorized_top_k/top_5_categorical_accuracy: 0.8126 - factorized_top_k/top_10_categorical_accuracy: 0.9255 - factorized_top_k/top_50_categorical_accuracy: 0.9976 - factorized_top_k/top_100_categorical_accuracy: 0.9999 - loss: 4595.5903 - regularization_loss: 0.0000e+00 - total_loss: 4595.5903\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 3s 763ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2173 - factorized_top_k/top_5_categorical_accuracy: 0.8177 - factorized_top_k/top_10_categorical_accuracy: 0.9279 - factorized_top_k/top_50_categorical_accuracy: 0.9965 - factorized_top_k/top_100_categorical_accuracy: 0.9999 - loss: 4525.9192 - regularization_loss: 0.0000e+00 - total_loss: 4525.9192\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 3s 761ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2154 - factorized_top_k/top_5_categorical_accuracy: 0.8189 - factorized_top_k/top_10_categorical_accuracy: 0.9290 - factorized_top_k/top_50_categorical_accuracy: 0.9978 - factorized_top_k/top_100_categorical_accuracy: 0.9999 - loss: 4486.5862 - regularization_loss: 0.0000e+00 - total_loss: 4486.5862\n",
      "1/1 [==============================] - 1s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 6.7981e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0039 - factorized_top_k/top_100_categorical_accuracy: 0.0066 - loss: 60111.1289 - regularization_loss: 0.0000e+00 - total_loss: 60111.1289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0006798096583224833,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.0038522547110915184,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.006571493111550808,\n",
       " 'loss': 60111.12890625,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 60111.12890625}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratings data.\n",
    "ratings = pd.read_csv('user_rating.csv')\n",
    "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings))\n",
    "\n",
    "# Split into train and test set in rating dataset\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(len(ratings), seed=42, reshuffle_each_iteration=False)\n",
    "ratings_train = shuffled.take(int(0.8 * len(ratings)))\n",
    "ratings_val = shuffled.skip(int(0.8 * len(ratings))).take(int(0.2 * len(ratings)))\n",
    "\n",
    "# Movies dataset\n",
    "movies = pd.read_csv('recipe_dataset.csv')\n",
    "movies = tf.data.Dataset.from_tensor_slices(dict(movies))\n",
    "\n",
    "# Select the basic features.\n",
    "ratings_train = ratings_train.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "ratings_val = ratings_val.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"Title\"])\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x:  tf.strings.as_string(x[\"User_id\"])))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies)\n",
    "\n",
    "class MovieLensModel(tfrs.Model):\n",
    "    # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "    # these are still plain Keras Models.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_model: tf.keras.Model,\n",
    "        movie_model: tf.keras.Model,\n",
    "        task: tfrs.tasks.Retrieval\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up user and movie representations.\n",
    "        self.user_model = user_model\n",
    "        self.movie_model = movie_model\n",
    "\n",
    "        # Set up a retrieval task.\n",
    "        self.task = task\n",
    "\n",
    "    def compute_loss(\n",
    "        self, features: Dict[Text, tf.Tensor], training=False\n",
    "    ) -> tf.Tensor:\n",
    "        # Define how the loss is computed.\n",
    "\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "        return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "# Define user and movie models.\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 64)\n",
    "])\n",
    "movie_model = tf.keras.Sequential([\n",
    "    movie_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 64)\n",
    "])\n",
    "\n",
    "# Define your objectives.\n",
    "task = tfrs.tasks.Retrieval(\n",
    "    metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates = movies.batch(128).map(movie_model)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a retrieval model.\n",
    "model = MovieLensModel(user_model, movie_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# fitting model to ratings_train dataset\n",
    "model.fit(ratings_train.batch(5000), epochs=20)\n",
    "\n",
    "# evaluate model to ratings_val dataset\n",
    "model.evaluate(ratings_val.batch(5000), return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Second Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "Epoch 1/500\n",
      "155/155 [==============================] - 20s 128ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 1.9421e-04 - factorized_top_k/top_10_categorical_accuracy: 5.1790e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0022 - factorized_top_k/top_100_categorical_accuracy: 0.0058 - loss: 456.9317 - regularization_loss: 4.1366 - total_loss: 461.0683 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 3.0211e-04 - val_factorized_top_k/top_10_categorical_accuracy: 3.0211e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0027 - val_factorized_top_k/top_100_categorical_accuracy: 0.0062 - val_loss: 59.9148 - val_regularization_loss: 0.2118 - val_total_loss: 60.1266\n",
      "Epoch 2/500\n",
      "155/155 [==============================] - 20s 130ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0030 - factorized_top_k/top_10_categorical_accuracy: 0.0062 - factorized_top_k/top_50_categorical_accuracy: 0.0350 - factorized_top_k/top_100_categorical_accuracy: 0.0591 - loss: 456.9305 - regularization_loss: 0.0609 - total_loss: 456.9914 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_10_categorical_accuracy: 1.5106e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0032 - val_factorized_top_k/top_100_categorical_accuracy: 0.0054 - val_loss: 59.9147 - val_regularization_loss: 0.0131 - val_total_loss: 59.9277\n",
      "Epoch 3/500\n",
      "155/155 [==============================] - 21s 134ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0033 - factorized_top_k/top_10_categorical_accuracy: 0.0062 - factorized_top_k/top_50_categorical_accuracy: 0.0302 - factorized_top_k/top_100_categorical_accuracy: 0.0543 - loss: 456.9327 - regularization_loss: 0.0040 - total_loss: 456.9367 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 3.0211e-04 - val_factorized_top_k/top_10_categorical_accuracy: 6.0423e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0026 - val_factorized_top_k/top_100_categorical_accuracy: 0.0059 - val_loss: 59.9146 - val_regularization_loss: 0.0014 - val_total_loss: 59.9160\n",
      "Epoch 4/500\n",
      "155/155 [==============================] - 21s 137ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0016 - factorized_top_k/top_10_categorical_accuracy: 0.0037 - factorized_top_k/top_50_categorical_accuracy: 0.0173 - factorized_top_k/top_100_categorical_accuracy: 0.0345 - loss: 456.9329 - regularization_loss: 4.0113e-04 - total_loss: 456.9333 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 4.5317e-04 - val_factorized_top_k/top_10_categorical_accuracy: 7.5529e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0027 - val_factorized_top_k/top_100_categorical_accuracy: 0.0076 - val_loss: 59.9146 - val_regularization_loss: 1.4533e-04 - val_total_loss: 59.9148\n",
      "Epoch 5/500\n",
      "155/155 [==============================] - 21s 138ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 8.4159e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0017 - factorized_top_k/top_50_categorical_accuracy: 0.0104 - factorized_top_k/top_100_categorical_accuracy: 0.0243 - loss: 456.9329 - regularization_loss: 4.4471e-05 - total_loss: 456.9330 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 1.5106e-04 - val_factorized_top_k/top_10_categorical_accuracy: 1.5106e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0026 - val_factorized_top_k/top_100_categorical_accuracy: 0.0071 - val_loss: 59.9146 - val_regularization_loss: 1.5888e-05 - val_total_loss: 59.9147\n",
      "Epoch 6/500\n",
      "155/155 [==============================] - 21s 138ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 7.1211e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0011 - factorized_top_k/top_50_categorical_accuracy: 0.0070 - factorized_top_k/top_100_categorical_accuracy: 0.0192 - loss: 456.9330 - regularization_loss: 5.2219e-06 - total_loss: 456.9330 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_50_categorical_accuracy: 0.0035 - val_factorized_top_k/top_100_categorical_accuracy: 0.0071 - val_loss: 59.9146 - val_regularization_loss: 1.9632e-06 - val_total_loss: 59.9146\n",
      "Epoch 7/500\n",
      "155/155 [==============================] - 21s 139ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 7.1211e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0010 - factorized_top_k/top_50_categorical_accuracy: 0.0054 - factorized_top_k/top_100_categorical_accuracy: 0.0157 - loss: 456.9330 - regularization_loss: 6.7038e-07 - total_loss: 456.9330 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 3.0211e-04 - val_factorized_top_k/top_10_categorical_accuracy: 3.0211e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0032 - val_factorized_top_k/top_100_categorical_accuracy: 0.0073 - val_loss: 59.9146 - val_regularization_loss: 2.7571e-07 - val_total_loss: 59.9146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x169a92a5d90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratings data.\n",
    "ratings = pd.read_csv('user_rating.csv')\n",
    "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings))\n",
    "\n",
    "# Split into train and test set in rating dataset\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(len(ratings), seed=42, reshuffle_each_iteration=False)\n",
    "ratings_train = shuffled.take(int(0.7 * len(ratings)))\n",
    "ratings_val = shuffled.skip(int(0.7 * len(ratings))).take(int(0.3 * len(ratings)))\n",
    "\n",
    "# Movies dataset\n",
    "movies = pd.read_csv('recipe_dataset.csv')\n",
    "movies = tf.data.Dataset.from_tensor_slices(dict(movies))\n",
    "\n",
    "# Select the basic features.\n",
    "ratings_train = ratings_train.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "ratings_val = ratings_val.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"Title\"])\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x: tf.strings.as_string(x[\"User_id\"])))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies)\n",
    "\n",
    "class MovieLensModel(tfrs.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_model: tf.keras.Model,\n",
    "        movie_model: tf.keras.Model,\n",
    "        task: tfrs.tasks.Retrieval\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_model = user_model\n",
    "        self.movie_model = movie_model\n",
    "        self.task = task\n",
    "\n",
    "    def compute_loss(\n",
    "        self, features: Dict[Text, tf.Tensor], training=False\n",
    "    ) -> tf.Tensor:\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "        return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "# Define user and movie models.\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 128, embeddings_regularizer=tf.keras.regularizers.l2(0.01)),  # Increased embedding size\n",
    "     # add l2 regulazation\n",
    "    tf.keras.layers.Dropout(0.2) # Added dropout regularization\n",
    "])\n",
    "movie_model = tf.keras.Sequential([\n",
    "    movie_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 128, embeddings_regularizer=tf.keras.regularizers.l2(0.01)),  # Increased embedding size\n",
    "    # add l2 regulazation\n",
    "    tf.keras.layers.Dropout(0.2) # Added dropout regularization\n",
    "])\n",
    "\n",
    "# Define your objectives.\n",
    "task = tfrs.tasks.Retrieval(\n",
    "    metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates=movies.batch(128).map(movie_model)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a retrieval model.\n",
    "model = MovieLensModel(user_model, movie_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# # Increase epochs for training\n",
    "# model.fit(ratings_train.batch(5000), epochs=500)  # Increased number of epochs\n",
    "\n",
    "# # Evaluate model on validation set\n",
    "# model.evaluate(ratings_val.batch(5000), return_dict=True)\n",
    "\n",
    "# Add early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_factorized_top_k/top_5_categorical_accuracy',\n",
    "    patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore the weights from the epoch with the lowest validation loss\n",
    ")\n",
    "\n",
    "# fitting model\n",
    "model.fit(ratings_train.batch(100), validation_data=ratings_val.batch(100), epochs=500, callbacks=[early_stopping])\n",
    "\n",
    "# Additional suggestions for experimentation:\n",
    "# - Hyperparameter tuning\n",
    "# - Data preprocessing techniques\n",
    "# - Regularization techniques\n",
    "# - Trying different loss functions\n",
    "# - Ensemble methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYBdm35HpSKh"
   },
   "source": [
    "## **User regulazation L2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67klQoX0pWXw",
    "outputId": "3293a65e-d7cb-46e9-c83d-cd9aa8e7af8b"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 40s 9s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 5.6644e-05 - factorized_top_k/top_10_categorical_accuracy: 5.6644e-05 - factorized_top_k/top_50_categorical_accuracy: 5.0980e-04 - factorized_top_k/top_100_categorical_accuracy: 0.0011 - loss: 33928.6438 - regularization_loss: 61.5987 - total_loss: 33990.2422\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 41s 10s/step - factorized_top_k/top_1_categorical_accuracy: 0.0065 - factorized_top_k/top_5_categorical_accuracy: 0.0368 - factorized_top_k/top_10_categorical_accuracy: 0.0624 - factorized_top_k/top_50_categorical_accuracy: 0.1520 - factorized_top_k/top_100_categorical_accuracy: 0.2069 - loss: 30240.4160 - regularization_loss: 619.9056 - total_loss: 30860.3211\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 36s 9s/step - factorized_top_k/top_1_categorical_accuracy: 0.0490 - factorized_top_k/top_5_categorical_accuracy: 0.2848 - factorized_top_k/top_10_categorical_accuracy: 0.4394 - factorized_top_k/top_50_categorical_accuracy: 0.6882 - factorized_top_k/top_100_categorical_accuracy: 0.7552 - loss: 20678.4227 - regularization_loss: 1849.2427 - total_loss: 22527.6652\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 37s 9s/step - factorized_top_k/top_1_categorical_accuracy: 0.1210 - factorized_top_k/top_5_categorical_accuracy: 0.5543 - factorized_top_k/top_10_categorical_accuracy: 0.6821 - factorized_top_k/top_50_categorical_accuracy: 0.8558 - factorized_top_k/top_100_categorical_accuracy: 0.8938 - loss: 14176.2352 - regularization_loss: 2426.9983 - total_loss: 16603.2336\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 36s 9s/step - factorized_top_k/top_1_categorical_accuracy: 0.1881 - factorized_top_k/top_5_categorical_accuracy: 0.6339 - factorized_top_k/top_10_categorical_accuracy: 0.7516 - factorized_top_k/top_50_categorical_accuracy: 0.8998 - factorized_top_k/top_100_categorical_accuracy: 0.9270 - loss: 12433.9268 - regularization_loss: 2486.1226 - total_loss: 14920.0496\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 38s 9s/step - factorized_top_k/top_1_categorical_accuracy: 0.2169 - factorized_top_k/top_5_categorical_accuracy: 0.6815 - factorized_top_k/top_10_categorical_accuracy: 0.8131 - factorized_top_k/top_50_categorical_accuracy: 0.9466 - factorized_top_k/top_100_categorical_accuracy: 0.9662 - loss: 9654.2494 - regularization_loss: 2463.9611 - total_loss: 12118.2105\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 36s 9s/step - factorized_top_k/top_1_categorical_accuracy: 0.2148 - factorized_top_k/top_5_categorical_accuracy: 0.7304 - factorized_top_k/top_10_categorical_accuracy: 0.8593 - factorized_top_k/top_50_categorical_accuracy: 0.9755 - factorized_top_k/top_100_categorical_accuracy: 0.9873 - loss: 7843.0240 - regularization_loss: 2414.7714 - total_loss: 10257.7955\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 36s 9s/step - factorized_top_k/top_1_categorical_accuracy: 0.1979 - factorized_top_k/top_5_categorical_accuracy: 0.7556 - factorized_top_k/top_10_categorical_accuracy: 0.8820 - factorized_top_k/top_50_categorical_accuracy: 0.9871 - factorized_top_k/top_100_categorical_accuracy: 0.9959 - loss: 6995.3670 - regularization_loss: 2354.5949 - total_loss: 9349.9619\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 36s 9s/step - factorized_top_k/top_1_categorical_accuracy: 0.1878 - factorized_top_k/top_5_categorical_accuracy: 0.7726 - factorized_top_k/top_10_categorical_accuracy: 0.8964 - factorized_top_k/top_50_categorical_accuracy: 0.9926 - factorized_top_k/top_100_categorical_accuracy: 0.9973 - loss: 6401.4461 - regularization_loss: 2307.0270 - total_loss: 8708.4730\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 36s 8s/step - factorized_top_k/top_1_categorical_accuracy: 0.1830 - factorized_top_k/top_5_categorical_accuracy: 0.7827 - factorized_top_k/top_10_categorical_accuracy: 0.9035 - factorized_top_k/top_50_categorical_accuracy: 0.9944 - factorized_top_k/top_100_categorical_accuracy: 0.9993 - loss: 5968.4783 - regularization_loss: 2268.0023 - total_loss: 8236.4805\n",
      "1/1 [==============================] - 9s 9s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 6.7981e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0034 - factorized_top_k/top_100_categorical_accuracy: 0.0075 - loss: 50115.6758 - regularization_loss: 2242.1597 - total_loss: 52357.8359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0006798096583224833,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.0033990482334047556,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.007477906066924334,\n",
       " 'loss': 50115.67578125,\n",
       " 'regularization_loss': 2242.15966796875,\n",
       " 'total_loss': 52357.8359375}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratings data.\n",
    "ratings = pd.read_csv('user_rating.csv')\n",
    "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings))\n",
    "\n",
    "# Split into train and test set in rating dataset\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(len(ratings), seed=42, reshuffle_each_iteration=False)\n",
    "ratings_train = shuffled.take(int(0.8 * len(ratings)))\n",
    "ratings_val = shuffled.skip(int(0.8 * len(ratings))).take(int(0.2 * len(ratings)))\n",
    "\n",
    "# Movies dataset\n",
    "movies = pd.read_csv('recipe_dataset.csv')\n",
    "movies = tf.data.Dataset.from_tensor_slices(dict(movies))\n",
    "\n",
    "# Select the basic features.\n",
    "ratings_train = ratings_train.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "ratings_val = ratings_val.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"Title\"])\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x:  tf.strings.as_string(x[\"User_id\"])))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies)\n",
    "\n",
    "class MovieLensModel(tfrs.Model):\n",
    "    # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "    # these are still plain Keras Models.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_model: tf.keras.Model,\n",
    "        movie_model: tf.keras.Model,\n",
    "        task: tfrs.tasks.Retrieval\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up user and movie representations.\n",
    "        self.user_model = user_model\n",
    "        self.movie_model = movie_model\n",
    "\n",
    "        # Set up a retrieval task.\n",
    "        self.task = task\n",
    "\n",
    "    def compute_loss(\n",
    "        self, features: Dict[Text, tf.Tensor], training=False\n",
    "    ) -> tf.Tensor:\n",
    "        # Define how the loss is computed.\n",
    "\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "        return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "# Define user and movie models.\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 64, embeddings_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "])\n",
    "movie_model = tf.keras.Sequential([\n",
    "    movie_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 64, embeddings_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "])\n",
    "\n",
    "# Define your objectives.\n",
    "task = tfrs.tasks.Retrieval(\n",
    "    metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates = movies.batch(128).map(movie_model)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a retrieval model.\n",
    "model = MovieLensModel(user_model, movie_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# fitting model to ratings_train dataset\n",
    "model.fit(ratings_train.batch(5000), epochs=10)\n",
    "\n",
    "# evaluate model to ratings_val dataset\n",
    "model.evaluate(ratings_val.batch(5000), return_dict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9t7BegBrhv1"
   },
   "source": [
    "## **Dropout Layers & early stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYoYmPmarqxO",
    "outputId": "dab57345-98b3-445d-c9f8-0186bdaa8f16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 5.6644e-05 - factorized_top_k/top_100_categorical_accuracy: 5.0980e-04 - loss: 80277.4609 - regularization_loss: 23.0544 - total_loss: 80300.5156 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f26fa0e8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 56s 30s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 5.6644e-05 - factorized_top_k/top_100_categorical_accuracy: 5.0980e-04 - loss: 76335.6875 - regularization_loss: 27.2037 - total_loss: 76362.8906 - val_factorized_top_k/top_1_categorical_accuracy: 2.2660e-04 - val_factorized_top_k/top_5_categorical_accuracy: 9.0641e-04 - val_factorized_top_k/top_10_categorical_accuracy: 0.0016 - val_factorized_top_k/top_50_categorical_accuracy: 0.0029 - val_factorized_top_k/top_100_categorical_accuracy: 0.0066 - val_loss: 37052.5195 - val_regularization_loss: 102.1569 - val_total_loss: 37154.6758\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 49s 26s/step - factorized_top_k/top_1_categorical_accuracy: 2.8322e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0046 - factorized_top_k/top_10_categorical_accuracy: 0.0119 - factorized_top_k/top_50_categorical_accuracy: 0.0579 - factorized_top_k/top_100_categorical_accuracy: 0.0975 - loss: 70643.9609 - regularization_loss: 307.1421 - total_loss: 70951.1016 - val_factorized_top_k/top_1_categorical_accuracy: 2.2660e-04 - val_factorized_top_k/top_5_categorical_accuracy: 6.7981e-04 - val_factorized_top_k/top_10_categorical_accuracy: 6.7981e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0036 - val_factorized_top_k/top_100_categorical_accuracy: 0.0061 - val_loss: 41825.3477 - val_regularization_loss: 1019.1355 - val_total_loss: 42844.4844\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 46s 25s/step - factorized_top_k/top_1_categorical_accuracy: 0.0480 - factorized_top_k/top_5_categorical_accuracy: 0.2779 - factorized_top_k/top_10_categorical_accuracy: 0.4299 - factorized_top_k/top_50_categorical_accuracy: 0.6541 - factorized_top_k/top_100_categorical_accuracy: 0.7463 - loss: 51776.0651 - regularization_loss: 1650.0837 - total_loss: 53426.1497 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 2.2660e-04 - val_factorized_top_k/top_10_categorical_accuracy: 4.5321e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0032 - val_factorized_top_k/top_100_categorical_accuracy: 0.0075 - val_loss: 52995.5742 - val_regularization_loss: 2376.2166 - val_total_loss: 55371.7891\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 44s 24s/step - factorized_top_k/top_1_categorical_accuracy: 0.1060 - factorized_top_k/top_5_categorical_accuracy: 0.5453 - factorized_top_k/top_10_categorical_accuracy: 0.6500 - factorized_top_k/top_50_categorical_accuracy: 0.7817 - factorized_top_k/top_100_categorical_accuracy: 0.8215 - loss: 40366.4453 - regularization_loss: 2610.8049 - total_loss: 42977.2487 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 2.2660e-04 - val_factorized_top_k/top_10_categorical_accuracy: 6.7981e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0025 - val_factorized_top_k/top_100_categorical_accuracy: 0.0086 - val_loss: 56883.2930 - val_regularization_loss: 2809.4707 - val_total_loss: 59692.7656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26fa0de830>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratings data.\n",
    "ratings = pd.read_csv('user_rating.csv')\n",
    "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings))\n",
    "\n",
    "# Split into train and test set in rating dataset\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(len(ratings), seed=42, reshuffle_each_iteration=False)\n",
    "ratings_train = shuffled.take(int(0.8 * len(ratings)))\n",
    "ratings_val = shuffled.skip(int(0.8 * len(ratings))).take(int(0.2 * len(ratings)))\n",
    "\n",
    "# Movies dataset\n",
    "movies = pd.read_csv('recipe_dataset.csv')\n",
    "movies = tf.data.Dataset.from_tensor_slices(dict(movies))\n",
    "\n",
    "# Select the basic features.\n",
    "ratings_train = ratings_train.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "ratings_val = ratings_val.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"Title\"])\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x:  tf.strings.as_string(x[\"User_id\"])))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies)\n",
    "\n",
    "class MovieLensModel(tfrs.Model):\n",
    "    # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "    # these are still plain Keras Models.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_model: tf.keras.Model,\n",
    "        movie_model: tf.keras.Model,\n",
    "        task: tfrs.tasks.Retrieval\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up user and movie representations.\n",
    "        self.user_model = user_model\n",
    "        self.movie_model = movie_model\n",
    "\n",
    "        # Set up a retrieval task.\n",
    "        self.task = task\n",
    "\n",
    "    def compute_loss(\n",
    "        self, features: Dict[Text, tf.Tensor], training=False\n",
    "    ) -> tf.Tensor:\n",
    "        # Define how the loss is computed.\n",
    "\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "        return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "# Define user and movie models.\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 64, embeddings_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.2)\n",
    "])\n",
    "movie_model = tf.keras.Sequential([\n",
    "    movie_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 64, embeddings_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.2)\n",
    "])\n",
    "\n",
    "# Define your objectives.\n",
    "task = tfrs.tasks.Retrieval(\n",
    "    metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates = movies.batch(128).map(movie_model)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a retrieval model.\n",
    "model = MovieLensModel(user_model, movie_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# Add early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore the weights from the epoch with the lowest validation loss\n",
    ")\n",
    "\n",
    "# fitting model\n",
    "model.fit(ratings_train.batch(10_000), validation_data=ratings_val.batch(10_000), epochs=10, callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrN5PeNbuUpm"
   },
   "source": [
    "## **Model complexity**\n",
    "add dense layer and increase embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvT5LdPwukrr",
    "outputId": "53a86626-60d6-4bf5-bba5-da5995862095"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 80306.1641 - regularization_loss: 24.4910 - total_loss: 80330.6562 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_test_function.<locals>.test_function at 0x7f26f889c5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 46s 23s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 76373.8802 - regularization_loss: 25.5880 - total_loss: 76399.4688 - val_factorized_top_k/top_1_categorical_accuracy: 4.5321e-04 - val_factorized_top_k/top_5_categorical_accuracy: 4.5321e-04 - val_factorized_top_k/top_10_categorical_accuracy: 0.0014 - val_factorized_top_k/top_50_categorical_accuracy: 0.0057 - val_factorized_top_k/top_100_categorical_accuracy: 0.0084 - val_loss: 135155.6250 - val_regularization_loss: 100.6231 - val_total_loss: 135256.2500\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 43s 23s/step - factorized_top_k/top_1_categorical_accuracy: 1.6993e-04 - factorized_top_k/top_5_categorical_accuracy: 1.6993e-04 - factorized_top_k/top_10_categorical_accuracy: 1.6993e-04 - factorized_top_k/top_50_categorical_accuracy: 4.5316e-04 - factorized_top_k/top_100_categorical_accuracy: 8.4967e-04 - loss: 2748985.2083 - regularization_loss: 2187.5802 - total_loss: 2751172.7500 - val_factorized_top_k/top_1_categorical_accuracy: 2.2660e-04 - val_factorized_top_k/top_5_categorical_accuracy: 4.5321e-04 - val_factorized_top_k/top_10_categorical_accuracy: 4.5321e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0029 - val_factorized_top_k/top_100_categorical_accuracy: 0.0061 - val_loss: 9298181.0000 - val_regularization_loss: 5167.4023 - val_total_loss: 9303348.0000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 43s 25s/step - factorized_top_k/top_1_categorical_accuracy: 2.2658e-04 - factorized_top_k/top_5_categorical_accuracy: 6.7973e-04 - factorized_top_k/top_10_categorical_accuracy: 7.9302e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0069 - factorized_top_k/top_100_categorical_accuracy: 0.0143 - loss: 25273567.3333 - regularization_loss: 8625.6491 - total_loss: 25282192.6667 - val_factorized_top_k/top_1_categorical_accuracy: 0.0014 - val_factorized_top_k/top_5_categorical_accuracy: 0.0014 - val_factorized_top_k/top_10_categorical_accuracy: 0.0014 - val_factorized_top_k/top_50_categorical_accuracy: 0.0034 - val_factorized_top_k/top_100_categorical_accuracy: 0.0054 - val_loss: 41290080.0000 - val_regularization_loss: 14265.8594 - val_total_loss: 41304344.0000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 42s 24s/step - factorized_top_k/top_1_categorical_accuracy: 0.0061 - factorized_top_k/top_5_categorical_accuracy: 0.0093 - factorized_top_k/top_10_categorical_accuracy: 0.0099 - factorized_top_k/top_50_categorical_accuracy: 0.0311 - factorized_top_k/top_100_categorical_accuracy: 0.0703 - loss: 55929829.3333 - regularization_loss: 15652.5104 - total_loss: 55945480.0000 - val_factorized_top_k/top_1_categorical_accuracy: 0.0390 - val_factorized_top_k/top_5_categorical_accuracy: 0.0392 - val_factorized_top_k/top_10_categorical_accuracy: 0.0392 - val_factorized_top_k/top_50_categorical_accuracy: 0.0437 - val_factorized_top_k/top_100_categorical_accuracy: 0.0467 - val_loss: 8337363.5000 - val_regularization_loss: 17743.0938 - val_total_loss: 8355106.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26f99b1150>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratings data.\n",
    "ratings = pd.read_csv('user_rating.csv')\n",
    "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings))\n",
    "\n",
    "# Split into train and test set in rating dataset\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(len(ratings), seed=42, reshuffle_each_iteration=False)\n",
    "ratings_train = shuffled.take(int(0.8 * len(ratings)))\n",
    "ratings_val = shuffled.skip(int(0.8 * len(ratings))).take(int(0.2 * len(ratings)))\n",
    "\n",
    "# Movies dataset\n",
    "movies = pd.read_csv('recipe_dataset.csv')\n",
    "movies = tf.data.Dataset.from_tensor_slices(dict(movies))\n",
    "\n",
    "# Select the basic features.\n",
    "ratings_train = ratings_train.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "ratings_val = ratings_val.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"Title\"])\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x:  tf.strings.as_string(x[\"User_id\"])))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies)\n",
    "\n",
    "class MovieLensModel(tfrs.Model):\n",
    "    # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "    # these are still plain Keras Models.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_model: tf.keras.Model,\n",
    "        movie_model: tf.keras.Model,\n",
    "        task: tfrs.tasks.Retrieval\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up user and movie representations.\n",
    "        self.user_model = user_model\n",
    "        self.movie_model = movie_model\n",
    "\n",
    "        # Set up a retrieval task.\n",
    "        self.task = task\n",
    "\n",
    "    def compute_loss(\n",
    "        self, features: Dict[Text, tf.Tensor], training=False\n",
    "    ) -> tf.Tensor:\n",
    "        # Define how the loss is computed.\n",
    "\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "        return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "# Define user and movie models.\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 128, embeddings_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2)\n",
    "])\n",
    "movie_model = tf.keras.Sequential([\n",
    "    movie_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 128, embeddings_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2)\n",
    "])\n",
    "\n",
    "# Define your objectives.\n",
    "task = tfrs.tasks.Retrieval(\n",
    "    metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates = movies.batch(128).map(movie_model)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a retrieval model.\n",
    "model = MovieLensModel(user_model, movie_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# Add early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore the weights from the epoch with the lowest validation loss\n",
    ")\n",
    "\n",
    "# fitting model\n",
    "model.fit(ratings_train.batch(10_000), validation_data=ratings_val.batch(10_000), epochs=10, callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dD0c0bekwVaA"
   },
   "source": [
    "## **Change learning rate**\n",
    "Change dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gJm0QzQwhrO",
    "outputId": "ba7a5879-01b3-4047-8c6f-51fea529c4ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 46s 24s/step - factorized_top_k/top_1_categorical_accuracy: 1.1329e-04 - factorized_top_k/top_5_categorical_accuracy: 2.8322e-04 - factorized_top_k/top_10_categorical_accuracy: 9.0631e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0033 - factorized_top_k/top_100_categorical_accuracy: 0.0067 - loss: 76334.0521 - regularization_loss: 21.8412 - total_loss: 76355.8958 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 6.7981e-04 - val_factorized_top_k/top_10_categorical_accuracy: 6.7981e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0045 - val_factorized_top_k/top_100_categorical_accuracy: 0.0079 - val_loss: 37035.4805 - val_regularization_loss: 21.8084 - val_total_loss: 37057.2891\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 45s 24s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 1.6993e-04 - factorized_top_k/top_10_categorical_accuracy: 2.2658e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0012 - factorized_top_k/top_100_categorical_accuracy: 0.0023 - loss: 76323.1302 - regularization_loss: 21.8022 - total_loss: 76344.9297 - val_factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - val_factorized_top_k/top_5_categorical_accuracy: 6.7981e-04 - val_factorized_top_k/top_10_categorical_accuracy: 0.0014 - val_factorized_top_k/top_50_categorical_accuracy: 0.0036 - val_factorized_top_k/top_100_categorical_accuracy: 0.0073 - val_loss: 37035.6328 - val_regularization_loss: 21.7884 - val_total_loss: 37057.4219\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 46s 25s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 5.6644e-05 - factorized_top_k/top_10_categorical_accuracy: 2.8322e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0015 - factorized_top_k/top_100_categorical_accuracy: 0.0035 - loss: 76305.4062 - regularization_loss: 21.7971 - total_loss: 76327.2057 - val_factorized_top_k/top_1_categorical_accuracy: 2.2660e-04 - val_factorized_top_k/top_5_categorical_accuracy: 4.5321e-04 - val_factorized_top_k/top_10_categorical_accuracy: 6.7981e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0032 - val_factorized_top_k/top_100_categorical_accuracy: 0.0079 - val_loss: 37035.7383 - val_regularization_loss: 21.8129 - val_total_loss: 37057.5508\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 47s 26s/step - factorized_top_k/top_1_categorical_accuracy: 5.6644e-05 - factorized_top_k/top_5_categorical_accuracy: 2.2658e-04 - factorized_top_k/top_10_categorical_accuracy: 3.3987e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0027 - factorized_top_k/top_100_categorical_accuracy: 0.0066 - loss: 76275.7500 - regularization_loss: 21.8457 - total_loss: 76297.5938 - val_factorized_top_k/top_1_categorical_accuracy: 4.5321e-04 - val_factorized_top_k/top_5_categorical_accuracy: 9.0641e-04 - val_factorized_top_k/top_10_categorical_accuracy: 0.0014 - val_factorized_top_k/top_50_categorical_accuracy: 0.0036 - val_factorized_top_k/top_100_categorical_accuracy: 0.0082 - val_loss: 37035.8867 - val_regularization_loss: 21.9094 - val_total_loss: 37057.7969\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 47s 25s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 2.2658e-04 - factorized_top_k/top_10_categorical_accuracy: 7.3638e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0051 - factorized_top_k/top_100_categorical_accuracy: 0.0108 - loss: 76226.4323 - regularization_loss: 21.9808 - total_loss: 76248.4115 - val_factorized_top_k/top_1_categorical_accuracy: 6.7981e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0011 - val_factorized_top_k/top_10_categorical_accuracy: 0.0014 - val_factorized_top_k/top_50_categorical_accuracy: 0.0050 - val_factorized_top_k/top_100_categorical_accuracy: 0.0077 - val_loss: 37036.2266 - val_regularization_loss: 22.1212 - val_total_loss: 37058.3477\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 48s 27s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 3.9651e-04 - factorized_top_k/top_10_categorical_accuracy: 9.6295e-04 - factorized_top_k/top_50_categorical_accuracy: 0.0066 - factorized_top_k/top_100_categorical_accuracy: 0.0140 - loss: 76142.3073 - regularization_loss: 22.2539 - total_loss: 76164.5625 - val_factorized_top_k/top_1_categorical_accuracy: 9.0641e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0014 - val_factorized_top_k/top_10_categorical_accuracy: 0.0016 - val_factorized_top_k/top_50_categorical_accuracy: 0.0048 - val_factorized_top_k/top_100_categorical_accuracy: 0.0077 - val_loss: 37036.9648 - val_regularization_loss: 22.5166 - val_total_loss: 37059.4805\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 48s 27s/step - factorized_top_k/top_1_categorical_accuracy: 5.6644e-05 - factorized_top_k/top_5_categorical_accuracy: 2.8322e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0011 - factorized_top_k/top_50_categorical_accuracy: 0.0078 - factorized_top_k/top_100_categorical_accuracy: 0.0183 - loss: 75994.6771 - regularization_loss: 22.7466 - total_loss: 76017.4219 - val_factorized_top_k/top_1_categorical_accuracy: 0.0014 - val_factorized_top_k/top_5_categorical_accuracy: 0.0014 - val_factorized_top_k/top_10_categorical_accuracy: 0.0014 - val_factorized_top_k/top_50_categorical_accuracy: 0.0041 - val_factorized_top_k/top_100_categorical_accuracy: 0.0073 - val_loss: 37038.6602 - val_regularization_loss: 23.2025 - val_total_loss: 37061.8633\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 48s 27s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 1.1329e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0010 - factorized_top_k/top_50_categorical_accuracy: 0.0088 - factorized_top_k/top_100_categorical_accuracy: 0.0225 - loss: 75730.7448 - regularization_loss: 23.5816 - total_loss: 75754.3281 - val_factorized_top_k/top_1_categorical_accuracy: 0.0014 - val_factorized_top_k/top_5_categorical_accuracy: 0.0014 - val_factorized_top_k/top_10_categorical_accuracy: 0.0014 - val_factorized_top_k/top_50_categorical_accuracy: 0.0045 - val_factorized_top_k/top_100_categorical_accuracy: 0.0059 - val_loss: 37042.8711 - val_regularization_loss: 24.3289 - val_total_loss: 37067.1992\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 46s 25s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 1.1329e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0013 - factorized_top_k/top_50_categorical_accuracy: 0.0112 - factorized_top_k/top_100_categorical_accuracy: 0.0271 - loss: 75257.6641 - regularization_loss: 24.9235 - total_loss: 75282.5859 - val_factorized_top_k/top_1_categorical_accuracy: 0.0018 - val_factorized_top_k/top_5_categorical_accuracy: 0.0018 - val_factorized_top_k/top_10_categorical_accuracy: 0.0018 - val_factorized_top_k/top_50_categorical_accuracy: 0.0048 - val_factorized_top_k/top_100_categorical_accuracy: 0.0063 - val_loss: 37053.9102 - val_regularization_loss: 26.0808 - val_total_loss: 37079.9922\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 48s 27s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 1.1329e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0015 - factorized_top_k/top_50_categorical_accuracy: 0.0157 - factorized_top_k/top_100_categorical_accuracy: 0.0351 - loss: 74432.9792 - regularization_loss: 26.9610 - total_loss: 74459.9375 - val_factorized_top_k/top_1_categorical_accuracy: 0.0011 - val_factorized_top_k/top_5_categorical_accuracy: 0.0011 - val_factorized_top_k/top_10_categorical_accuracy: 0.0014 - val_factorized_top_k/top_50_categorical_accuracy: 0.0041 - val_factorized_top_k/top_100_categorical_accuracy: 0.0068 - val_loss: 37085.9570 - val_regularization_loss: 28.6427 - val_total_loss: 37114.6016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26f8d4ea10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratings data.\n",
    "ratings = pd.read_csv('user_rating.csv')\n",
    "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings))\n",
    "\n",
    "# Split into train and test set in rating dataset\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(len(ratings), seed=42, reshuffle_each_iteration=False)\n",
    "ratings_train = shuffled.take(int(0.8 * len(ratings)))\n",
    "ratings_val = shuffled.skip(int(0.8 * len(ratings))).take(int(0.2 * len(ratings)))\n",
    "\n",
    "# Movies dataset\n",
    "movies = pd.read_csv('recipe_dataset.csv')\n",
    "movies = tf.data.Dataset.from_tensor_slices(dict(movies))\n",
    "\n",
    "# Select the basic features.\n",
    "ratings_train = ratings_train.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "ratings_val = ratings_val.map(lambda x: {\n",
    "    \"movie_title\": x[\"Title\"],\n",
    "    \"user_id\": tf.strings.as_string(x[\"User_id\"])\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"Title\"])\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x:  tf.strings.as_string(x[\"User_id\"])))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies)\n",
    "\n",
    "class MovieLensModel(tfrs.Model):\n",
    "    # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "    # these are still plain Keras Models.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_model: tf.keras.Model,\n",
    "        movie_model: tf.keras.Model,\n",
    "        task: tfrs.tasks.Retrieval\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up user and movie representations.\n",
    "        self.user_model = user_model\n",
    "        self.movie_model = movie_model\n",
    "\n",
    "        # Set up a retrieval task.\n",
    "        self.task = task\n",
    "\n",
    "    def compute_loss(\n",
    "        self, features: Dict[Text, tf.Tensor], training=False\n",
    "    ) -> tf.Tensor:\n",
    "        # Define how the loss is computed.\n",
    "\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "        return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "# Define user and movie models.\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 132, embeddings_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3)\n",
    "])\n",
    "movie_model = tf.keras.Sequential([\n",
    "    movie_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 132, embeddings_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3)\n",
    "])\n",
    "\n",
    "# Define your objectives.\n",
    "task = tfrs.tasks.Retrieval(\n",
    "    metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates = movies.batch(128).map(movie_model)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a retrieval model.\n",
    "model = MovieLensModel(user_model, movie_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.01))\n",
    "\n",
    "# Add early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_factorized_top_k/top_5_categorical_accuracy',\n",
    "    patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore the weights from the epoch with the lowest validation loss\n",
    ")\n",
    "\n",
    "# fitting model\n",
    "model.fit(ratings_train.batch(10_000), validation_data=ratings_val.batch(10_000), epochs=10, callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
